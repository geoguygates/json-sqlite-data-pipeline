Design:
------
-I used a functional core, imperative shell design to break out the functions from the main top-level program code.

-The first top-level code file is called "extract_main.py" and implements the simple logic to extract the data from it's 
 source formats (json and sqlite) and perform the necessary data wrangling to reformat and standardize the data as four seperate pandas dataframes.
 The four pandas dataframes are then saved as parquet files to a folder called "extract_bucket".
 This step modularizes the pipeline and allows the developer to refactor and/or troubleshoot errors in the codebase more efficiently
 before starting the transformation and load steps of the ETL process.

-The second top-level code file is called "transform_load_main.py" in this step the code applies the necessary bussiness logic including joins and aggregation functions
 to create two of the three final analytics tables requested (transform) in the case study prompt (didn't get to the third)
 The two data tables are then loaded into a sqlite DB called "final_analytics.db". I intentionally chose code the logic so new data would be
 appended onto the already existing tables in "final_analytics.db" each time the pipeline ran so going forward we would have a historic log we could reference if
 the pipeline displayed irregular or incorrect behavior.


Possible Flaws:
---------------
-I need to write unit tests to ensure code quality.

-I need to deploy/automate my code, I can do this in a few ways depending on the importance of the pipeline.
	-Write a batch file and run on my local machine using task scheduler. (not ideal but could work for small jobs)
	-AWS this would be my preferred architecture and I have detailed exactly how I would do this in the "Future work" section below
	-Apache Airflow -admittedly I have never used this software but it seems to be an Octo favorite.

-Currently I would advise running the pipeline daily at 8:15 AM after the smart meter readings data come in however I would need to know more about the bussiness
logic to make this decision. I'm uncertain if there is a case where it would be useful to run the pipeline both after the sqlite db is updated and also after the smart meter readings come in.


Future Work / Pipeline Automation in AWS:
------------
-Ideally a pipeline like this would be deployed as a modular cloud architecture that could be more easily monitored and automated.

-You could achieve this by storing your raw source data (json and sqlite) in an S3 bucket and then deploying "extract_main.py" to a Lambda function which would run on
 a daily schedule at 8:15 AM the output tables from this lambda would be written 
 as parquet files to another S3 bucket called "extract_bucket" similar to how I have set this up on my local file directory. This S3 bucket would
 store intermediary standardized data tables from the extract step and would be read by another Lambda function running the "transform_load_main.py" code.
 The final analytics tables generated by the "transform_load_main.py" Lambda function could be sent to a final S3 bucket where a service like Athena
 could be used by the Analysts to query the parquet files. Or you could move the parquet files into an RDBMS product like Redshift or RDS
 where it would be readily queryable by data analysts.


